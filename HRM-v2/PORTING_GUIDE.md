# HRM-v2 Porting Guide

This document describes how the original HRM codebase has been modernized and restructured in HRM-v2.

## Architecture Changes

### Attention System

**Original (HRM-v1):**
- Direct import from `flash_attn` package
- No fallback mechanism
- Hardcoded FlashAttention 2/3 compatibility checks

**New (HRM-v2):**
- Unified attention wrapper in `src/hrm/ops/attention.py`
- Automatic fallback: FlashAttention 4 â†’ PyTorch SDPA
- Clean API: `attention(q, k, v, use_flash=True, is_causal=False)`
- Compatible with both sm_90 (H100) and sm_100 (Blackwell/RTX 5090)

### Module Organization

```
Original:                   HRM-v2:
models/                     src/hrm/
â”œâ”€â”€ common.py              â”œâ”€â”€ ops/           # Low-level operations
â”œâ”€â”€ layers.py              â”‚   â”œâ”€â”€ attention.py
â”œâ”€â”€ losses.py              â”‚   â”œâ”€â”€ rotary.py
â”œâ”€â”€ sparse_embedding.py    â”‚   â””â”€â”€ norm.py
â””â”€â”€ hrm/                   â”œâ”€â”€ models/        # Model architectures
    â””â”€â”€ hrm_act_v1.py      â”‚   â”œâ”€â”€ blocks.py
                           â”‚   â””â”€â”€ layers.py  # HRM-specific layers
                           â”œâ”€â”€ train/         # Training utilities
                           â””â”€â”€ utils/         # Helpers
                               â”œâ”€â”€ env.py
                               â””â”€â”€ init.py
```

## Ported Components

### âœ… Core Operations

| Component | Original | HRM-v2 Location | Status |
|-----------|----------|-----------------|--------|
| RoPE | `models/layers.py` | `src/hrm/ops/rotary.py` | âœ… Complete |
| RMS Norm | `models/layers.py` | `src/hrm/ops/norm.py` | âœ… Complete |
| Attention | `models/layers.py` | `src/hrm/ops/attention.py` | âœ… Modernized |
| Truncated Normal Init | `models/common.py` | `src/hrm/utils/init.py` | âœ… Complete |

### âœ… Layer Primitives

| Component | Original | HRM-v2 Location | Status |
|-----------|----------|-----------------|--------|
| CastedLinear | `models/layers.py` | `src/hrm/models/layers.py` | âœ… Complete |
| CastedEmbedding | `models/layers.py` | `src/hrm/models/layers.py` | âœ… Complete |
| SwiGLU | `models/layers.py` | `src/hrm/models/layers.py` | âœ… Complete |
| AttentionWithRoPE | `models/layers.py` | `src/hrm/models/layers.py` | âœ… Modernized |
| HRMTransformerBlock | `models/layers.py` | `src/hrm/models/layers.py` | âœ… Complete |

### ðŸ“‹ To Be Ported (As Needed)

| Component | Original | Status | Priority |
|-----------|----------|--------|----------|
| HRM-ACT-v1 Model | `models/hrm/hrm_act_v1.py` | Pending | Medium |
| Sparse Embedding | `models/sparse_embedding.py` | Pending | Low |
| Custom Losses | `models/losses.py` | Pending | Medium |
| Dataset Builders | `dataset/` | Pending | High |

## Key Improvements

### 1. Modern Python Packaging
- `pyproject.toml` with proper dependencies
- Editable install: `uv pip install -e .`
- Clean namespace: `from hrm.ops import attention`

### 2. FlashAttention 4 Support
- Targets Blackwell (sm_100) architecture
- Pinned to commit: `5c1627a7a1cda9c32cb9b937a053564e663f81bc`
- Build flags: `TORCH_CUDA_ARCH_LIST=10.0`

### 3. Environment Setup
- One-command setup: `bash scripts/setup_uv.sh`
- GPU verification: `python scripts/verify_gpu.py`
- Smoke tests: `bash scripts/smoke_train.sh`

### 4. Testing Infrastructure
- Comprehensive unit tests in `tests/`
- Pytest-based: `pytest tests/`
- GPU/CPU compatibility checks

### 5. Type Safety & Documentation
- Type hints throughout
- Docstrings for all public APIs
- Clear separation of concerns

## Migration Path

If you want to use the full HRM-ACT-v1 model:

1. **Port the main model** (when needed):
   ```python
   # Create src/hrm/models/hrm_act_v1.py
   # Adapt HierarchicalReasoningModel_ACTV1 to use new layers
   ```

2. **Port sparse embeddings** (if using puzzle embeddings):
   ```python
   # Create src/hrm/models/sparse_embedding.py
   # Update imports to use new base classes
   ```

3. **Port datasets**:
   ```python
   # Adapt dataset/ builders to work with new structure
   # Update paths and imports
   ```

## Dependencies

### Required
- Python 3.12+
- PyTorch 2.8+ (CUDA 12.8)
- NumPy, einops, PyYAML, tqdm, rich

### Optional
- FlashAttention 4 (Linux only; built from source)
- pytest, ruff (development)

## Build Instructions

### Standard Installation
```bash
cd HRM-v2
bash scripts/setup_uv.sh
```

### Manual FlashAttention 4 Build
```bash
export TORCH_CUDA_ARCH_LIST="10.0"  # Blackwell
export CMAKE_BUILD_PARALLEL_LEVEL=$(nproc)
uv pip install -v --no-binary=:all: --no-build-isolation \
  "git+https://github.com/Dao-AILab/flash-attention@5c1627a7a1cda9c32cb9b937a053564e663f81bc"
```

## Verification

After installation:
```bash
source .venv/bin/activate
python scripts/verify_gpu.py  # Check GPU and FlashAttention
pytest tests/                 # Run all tests
bash scripts/smoke_train.sh   # Quick end-to-end test
```

## Next Steps

The current HRM-v2 provides a clean foundation with:
- âœ… Modern attention infrastructure
- âœ… Core HRM layers (RoPE, RMS norm, SwiGLU, etc.)
- âœ… Unified FlashAttention 4 / SDPA interface
- âœ… Testing and verification tools

**To complete the port**, you can now:
1. Add the full HRM-ACT-v1 model when needed
2. Port dataset builders for your specific tasks
3. Implement training loops and evaluation scripts
4. Add task-specific components as required

The architecture is designed to be **extensible and minimal** - port only what you need!

