# Minimal configuration for testing HRM-v2

model:
  vocab_size: 10000
  embed_dim: 256
  num_heads: 8
  num_layers: 4
  ff_dim: 1024
  max_seqlen: 512
  dropout: 0.1
  use_flash: true

training:
  batch_size: 8
  learning_rate: 3.0e-4
  weight_decay: 0.01
  max_steps: 10000
  warmup_steps: 500
  gradient_clip: 1.0
  
  # Mixed precision
  dtype: bfloat16
  use_amp: true
  
  # Checkpointing
  checkpoint_every: 1000
  eval_every: 500
  log_every: 100

data:
  train_path: null
  val_path: null
  num_workers: 4
  max_seqlen: 512

optimizer:
  name: adamw
  betas: [0.9, 0.95]
  eps: 1.0e-8

scheduler:
  name: cosine
  warmup_steps: 500
  min_lr: 3.0e-5

